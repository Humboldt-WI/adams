{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XyuoS1V_A74Q"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Humboldt-WI/adams/blob/master/demos/nlp/word-2-vec.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "skvR6YA3A74T"
   },
   "source": [
    "# Word Embeddings and Word-to-Vec (W2V)\n",
    "This demo notebook revisits the lecture on word embeddings and Google's word-to-vec algorithm. W2V, like backpropagation, is a very popular algorithm that enjoys much coverage in various blogs, youtube channels, etc. In case you appreciate some additional material to read-up on W2V, here here are some useful resources including,  \n",
    "- [the original W2V paper](https://proceedings.neurips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf)\n",
    "- the beautiful [\"Illustrated Word2vec\" by Jay Alammar](https://jalammar.github.io/illustrated-word2vec/)\n",
    "- the[W2V Tensorflow tutorial](https://www.tensorflow.org/tutorials/text/word2vec)\n",
    "\n",
    "Last but not least, our main textbook features excellent chapters on word embeddings, W2V, and related algorithms inlcuding GloVe and Fasttext. You can find those parts in [Section 14 of Dive into Deep Learning](http://d2l.ai/chapter_natural-language-processing-pretraining/index.html)\n",
    "\n",
    "Let's get started with our ADAMS demo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GdTgrnLmA75V",
    "tags": []
   },
   "source": [
    "## Training word-to-vec embeddings\n",
    "When it comes to embeddings, the most common use case is to **download pre-trained embeddings** and employ these for some downstream tasks (with or without fine-tuning). The Keras *embedding layer* supports that use case very well, as we will see in a future demo on sentiment analysis. Since this demo aims at deepening our understanding of W2V, we focus on a different use case and demonstrate the training of **customer word embeddings** using our IMDB data. \n",
    "\n",
    "You could argue that the IMDB forum exhibits a specific type of speech or jargon, and that this justifies training word embeddings for this specific corpus. In practice, using pre-trained embeddings will almost surely give better results than training embeddings from zero. However, without going into too much detail of the pros and cons of pre-training your own embeddings versus employing pre-trained embeddings, perhaps with some finetuning, the point of this section is simply to showcase how you could train from scratch if you want to. To that end, we will use a library called `Gensim`. \n",
    "\n",
    "`Gensim` is a popular library for text processing. Although maybe even more geared toward topic modeling, it offers, among others, implementations of several algorithms to learn word embeddings including *W2V*, *GloVe*, and *Fasttext*. We demonstrate training W2V embeddings using our cleaned IMDB movie review data set. Before moving on, make sure to have installed `Gensim`. \n",
    "\n",
    "**Credits and disclaimers**: many of the examples you are going to see in this section have been inspired by this very nice [Kaggle post](https://www.kaggle.com/code/pierremegret/gensim-word2vec-tutorial/notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "60p3_IyJXfnZ"
   },
   "outputs": [],
   "source": [
    "# Create a global variable to idicate whether the notebook is run in Colab\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "# Configure variables pointing to directories and stored files \n",
    "if IN_COLAB:\n",
    "    # Mount Google-Drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    DATA_DIR = '/content/drive/My Drive/'  # adjust to Google drive folder with the data if applicable\n",
    "else:\n",
    "    DATA_DIR = './' # adjust to the directory where data is stored on your machine (if running the notebook locally)\n",
    "\n",
    "sys.path.append(DATA_DIR)\n",
    "\n",
    "CLEAN_REVIEW = DATA_DIR + 'imdb_clean_full_v2.pkl'   # List with tokenized reviews after standard NLP preparation\n",
    "IMBD_EMBEDDINGS = DATA_DIR + 'w2v_imdb_full_d100_e500.model'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NpBlcJOEtsFX"
   },
   "source": [
    "### Recap W2V\n",
    "Let's quickly revisit the principles of W2V. Please consult the paper of [Mikolov et al. (2013)](https://proceedings.neurips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf) for a detailed description.\n",
    "\n",
    "W2V establishes a word's meaning by the words that frequently appear close-by (distributional semantics). More specifically, the context of a word consists of the words that appear next to it within a pre-defined window (let's say 5 words).\n",
    "\n",
    " - the quality of *air* in mainland China has been decreasing since..\n",
    " - doctors claim the *air* you breath defines the overall wellbeing...\n",
    " - the currents of hot *air* have been bursting from underground\n",
    " - the mountain *air* was crystal clean and filled with ..\n",
    " - in case of *air* supply shortages, the submarine will..\n",
    "\n",
    "Taking the word *air* as our **target word**, the words around *air*, called context words, define the **meaning** of the word *air* in W2V.\n",
    "\n",
    "![w2vprocess](w2v.jpg)\n",
    "<br>\n",
    "inspired by https://www.youtube.com/watch?v=BD8wPsr_DAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4rmnnwM3UwrE",
    "tags": []
   },
   "source": [
    "### Loading the data\n",
    "We load the data frame with the original and cleaned reviews. The original version does not matter for this session. We will delete them to save memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 581,
     "status": "ok",
     "timestamp": 1649251541851,
     "user": {
      "displayName": "Georg B",
      "userId": "16990759077565939430"
     },
     "user_tz": -120
    },
    "id": "eP6mlAonUwrF",
    "outputId": "e8d45569-5eb4-4c76-b88c-8bc2a1641f85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 3 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   review        50000 non-null  object\n",
      " 1   sentiment     50000 non-null  object\n",
      " 2   review_clean  50000 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open(CLEAN_REVIEW,'rb') as path_name:\n",
    "    df = pickle.load(path_name)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "liLqBv7CA75V"
   },
   "source": [
    "### The Gensim W2V model\n",
    "Training word embeddings using `Gensim` is very easy and just a matter of calling a function. Well, the reason it takes so little code is that we have already cleaned our data and have it available as an array of texts; that is a format that `Gensim`supports. However, note that, depending on your data, the code may take quite a while to run. Again, word embeddings trained on the full 50K data set for 500 epochs are available in our course folder.\n",
    "\n",
    "Gensim is build for scalability. Would we use a large corpus, it were not be practical to first load all data from disk into your computer's main memory, to then process the data document by document using Gensim. Instead, it would be much more scalable to stream the data from disk. Long story short, we need a bit of infrastructure to input our review data set, which, for simplicity, we keep in a data frame, in a way that complies with what Gensim expects.  into. To that end, we build a little helper class that facilitates streaming reviews from our data frame. It would be easy to extend the helper class so as to facilitate streaming reviews from disk, or support both options. The [Gensim documentation](https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html#training-your-own-model) provides an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "FaXIniRYtsFg"
   },
   "outputs": [],
   "source": [
    "# Helper class to input reviews from our data frame into Gensim\n",
    "from gensim import utils\n",
    "\n",
    "class CleanReviews:\n",
    "    \"\"\"An iterator that yields sentences (lists of str).\"\"\"\n",
    "    \n",
    "    def __init__(self, reviews):\n",
    "        self.reviews = reviews\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for line in self.reviews:\n",
    "            yield utils.simple_preprocess(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZWo7-_xrtsFh"
   },
   "source": [
    "And here is the simple call to the function `Word2Vec` that trains our custom word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "HN4TnV8zA75W"
   },
   "outputs": [],
   "source": [
    "# CAUTION: Running the code might take a while\n",
    "from gensim.models import Word2Vec    \n",
    "\n",
    "emb_dim = 10  # embedding dimension, we use 10 for a quick demo of the code\n",
    "reviews = CleanReviews(df.review_clean)\n",
    "# Train a Word2Vec model\n",
    "model = Word2Vec(sentences=reviews, \n",
    "                 min_count=5,  # min_count means the word frequency threshold, if =2 and word is used only once - it's not included\n",
    "                 window=5,     # the size of context window\n",
    "                 epochs=5,     # epochs is set to 5 to decrease runtim, would be much larger in practice\n",
    "                 vector_size=emb_dim,  # size of embedding\n",
    "                 workers=2)    # for parallel computing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ErPuF2e4tsFj"
   },
   "source": [
    "Make sure to check out the docstring of the `Word2Vec` function to discover how word vectors are trained by default. Importantly, the argument `sg` let's you chose between *skip-gram* and *cbow*. Other concepts we discussed in the lecture include accelerating computations using *hierarchical softmax* and *negative sampling*. Gensim features these through its arguments `hs` and `negative`, respectively. Obviously, tons of other functionality is available, so make sure to study the [documentation](https://radimrehurek.com/gensim/models/word2vec.html?highlight=word2vec) if you plan to use the Gensim library for serious projects. Also, just to remind you, the [Kaggle post](https://www.kaggle.com/code/pierremegret/gensim-word2vec-tutorial/notebook), which inspired this notebook, has a slightly more elaborate demo of how to set up training and, specifically, how you can break down the individual steps of W2V training into smaller pieces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QbxcdDDStsFk"
   },
   "source": [
    "The trained word vectors are accessible through the field `wv` of the model class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "aEivzRgUtsFk",
    "outputId": "561b9212-a5f1-4493-b939-9377212bed5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.7333734  -0.8453565   1.6124092  -1.7317083   3.59629    -2.0243227\n",
      " -0.51750576  2.7008739  -0.6114781   3.265099  ]\n"
     ]
    }
   ],
   "source": [
    "# what is the word vector of the words good and bad?\n",
    "print(model.wv['good'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "yiotoAx0tsFl",
    "outputId": "f3a23ab8-79a6-448b-c230-c4c4e8293a14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.3933642   0.33083862  3.1319673  -1.321046    4.126081   -4.1444077\n",
      " -1.2530929   3.7277522  -0.22870462  1.1351136 ]\n"
     ]
    }
   ],
   "source": [
    "print(model.wv['bad'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "-IMFzRSFtsFm",
    "outputId": "0ad6e544-3a9a-49d6-ac58-c9e7b202ac65"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30201"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.wv.key_to_index)  # how many word vectors have been trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z1SaNZly6TfN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oeX0dD19tsFn"
   },
   "source": [
    "We continue with playing with word vectors shortly but let us first discuss input and output handling with Gensim."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dKjhdPPAA75X"
   },
   "source": [
    "### Input / output handling\n",
    "Gensim supports saving and loading of trained embeddings in different versions. This makes a lot of sense since training can take a long time. For example, you could train for a couple of epochs, then store your results on disk, and then continue training. Here is how we can store our trained word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Bo93egFsA75Y"
   },
   "outputs": [],
   "source": [
    "# Save trained word vectors to disk\n",
    "file=\"w2v_tmp.model\"\n",
    "save_as_bin = False\n",
    "model.wv.save_word2vec_format(file, binary=save_as_bin)  # set binary to True to save disk space; false facilitates inspecting the embeddings in a text editor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n_UX44R-tsFo"
   },
   "source": [
    "For Adams, you can obtain word vectors trained on the IMDB corpus for 500 epochs from our [GitHub repository](https://github.com/Humboldt-WI/adams/tree/master/demos/nlp). These vectors are far from comparable to real pre-trained W2V embeddings. On the other hand, their training took a couple of hours so the vectors should carry a bit more information compared to just running the above training code with a small embedding dimension of ten and training for only five epochs. Let's showcase how we can save and load word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "eD7dCexjA75Z"
   },
   "outputs": [],
   "source": [
    "# Load model from disk\n",
    "from gensim.models import KeyedVectors\n",
    "w2v = KeyedVectors.load_word2vec_format(IMBD_EMBEDDINGS, binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bNx2jzvatsFq"
   },
   "source": [
    "Remember that you can also access the `KeyedVectors`, which we load with the previous statement, directly from a trained model object via the field `wv`. Thus, if you would like to run the following demos with the word vectors you trained yourself, simply run the following command. One would expect that the demos give nicer results with the pre-trained embeddings from your repo but you are welcoem to try this our yourself. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "FPIZnAJXtsFq"
   },
   "outputs": [],
   "source": [
    "# w2v = model.wv  # continue with the W2V embeddings trained above "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZYt70SnBA75Z",
    "tags": []
   },
   "source": [
    "### Playing with embeddings\n",
    "Again, the embeddings loaded above are far from solid but should give us some somewhat meaningful results in algebraic comparisons. Let's see whether this works out. \n",
    "\n",
    "#### Which word is most similar to another word?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lewhU1G7A75a",
    "outputId": "ab90e32d-b95e-4f35-ef74-3b7ff671729b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('film', 0.7905927896499634),\n",
       " ('flick', 0.6631829142570496),\n",
       " ('really', 0.6255204081535339),\n",
       " ('honestly', 0.5946516990661621),\n",
       " ('sequel', 0.5812180042266846),\n",
       " ('thing', 0.5797049403190613),\n",
       " ('think', 0.5796433091163635),\n",
       " ('probably', 0.572991132736206),\n",
       " ('sure', 0.5676621794700623),\n",
       " ('watch', 0.5486831068992615)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar(positive=['movie'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7v4DJ1egA75b"
   },
   "source": [
    "#### How similar are two words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P22xPLE5A75c",
    "outputId": "ed2f54db-b147-4e8d-9036-aee3b1c3e5c2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.70135677"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.similarity('good', 'great')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "z5pxaVl8A75c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How similar is Tarantino to Rodriguez: 0.21523767709732056\n",
      "How similar is Tarantino to Spielberg: 0.3858705461025238\n"
     ]
    }
   ],
   "source": [
    "print('How similar is Tarantino to Rodriguez: {}'.format(w2v.similarity('tarantino', 'rodriguez')))\n",
    "print('How similar is Tarantino to Spielberg: {}'.format(w2v.similarity('tarantino', 'spielberg')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O0tZ0Jpq9eTJ"
   },
   "source": [
    "Clearly, the above result does not make sense. While we tried to pick names that would probably refer to the actual directors in a review, we can not be certain that this is the case. Especially the name *Rodriguez* is a problem in that regard since a mention of this name in a movie review could refer to Robert but also to Michelle Rodriguez, which might explain why we observe a counter-intuitive result. \n",
    "\n",
    "Disclaimer: no worries if the above does not make sense to you. Perhaps you just lack the *right taste* for movies ;)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nxlk22MJA75d"
   },
   "source": [
    "#### Which word does not fit in?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 93
    },
    "id": "JPYGN35DA75d",
    "outputId": "b974bffc-2b12-4523-fba7-2567de668023"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weak\n",
      "good\n"
     ]
    }
   ],
   "source": [
    "print(w2v.doesnt_match(['cool', 'great', 'lovely', 'weak']))\n",
    "print(w2v.doesnt_match(['movie', 'film', 'good']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "htZvGR4YA75e"
   },
   "source": [
    "#### A is to B as C is to ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h-DTdgxNA75e",
    "outputId": "6621430d-aafd-42a6-c198-51eabc845099"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('larraz', 0.437841534614563),\n",
       " ('tantrum', 0.38007310032844543),\n",
       " ('winslet', 0.38005053997039795),\n",
       " ('notting', 0.371686726808548),\n",
       " ('ideological', 0.35116255283355713)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar(positive=['spielberg', 'woman'], negative=['man'], topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QeiOWgJkA75h"
   },
   "source": [
    "### Phrase detection\n",
    "W2V trains one embedding per word. The model is agnostic of common phrases such as 'New York'. It would train one embedding for new and another for york, provided both words are part of the vocabulary. You can get better embeddings by adding common phrases to the vocabulary. W2V will then train individual embeddings for these phrases. Gensims also comes with a phrase detection models, which allows you to handle bigrams, trigrams and the like. We will not retrain our W2V model but sketch how you can use Gensim to get these common phrases. You could then consider to add (some of) them to your vocab and enhance the model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "JgFV1l0SA75h"
   },
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases, ENGLISH_CONNECTOR_WORDS\n",
    "# Train a bigram model\n",
    "bigram_model = Phrases(sentences=reviews,min_count=10 , threshold=1, connector_words=ENGLISH_CONNECTOR_WORDS) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EdmEVqvdA75h"
   },
   "source": [
    "After training, we can take text and put it through the bigram model. The model will then alter the text so as to introduce bigrams. Here is an example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AzZUbniKA75h",
    "outputId": "0edab39a-866b-4f3a-fcd0-5968295d2bb4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'like', 'this', 'movie']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to process text and replace phrases, we use our phrase detector as follows\n",
    "bigram_model['I', 'like', 'this', 'movie']  # no phrases to be detected here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aLZla_QUtsFx",
    "outputId": "0edab39a-866b-4f3a-fcd0-5968295d2bb4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sex', 'and', 'the', 'city', 'is', 'all', 'about', 'new_york']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_model['sex', 'and', 'the', 'city', 'is', 'all', 'about', 'new', 'york']  # but we would expect city names to be detected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wGbE6LWEA75j"
   },
   "source": [
    "We can also make use of our counter class to examine the most common bigrams in the corpus, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "JefH9K6TA75j"
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "bigram_counter = collections.Counter()\n",
    "for key in bigram_model.vocab.keys():\n",
    "    if key.find('_')>-1: # the decode is needed because Gensims stores keys as bytes\n",
    "        bigram_counter[key] += bigram_model.vocab[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "72LnWyn2A75k",
    "outputId": "8d0cc7c8-6c26-4ef3-a7a1-6707798e8631"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('look_like', 3715),\n",
       " ('watch_movie', 3121),\n",
       " ('ever_see', 2973),\n",
       " ('see_movie', 2752),\n",
       " ('bad_movie', 2727),\n",
       " ('make_movie', 2392),\n",
       " ('year_old', 2389),\n",
       " ('film_make', 2369),\n",
       " ('special_effect', 2308),\n",
       " ('movie_make', 2134),\n",
       " ('one_best', 2030),\n",
       " ('even_though', 1999),\n",
       " ('movie_ever', 1987),\n",
       " ('movie_like', 1921),\n",
       " ('low_budget', 1892),\n",
       " ('make_film', 1882),\n",
       " ('see_film', 1859),\n",
       " ('main_character', 1838),\n",
       " ('waste_time', 1793),\n",
       " ('watch_film', 1664),\n",
       " ('good_movie', 1634),\n",
       " ('horror_movie', 1611),\n",
       " ('much_well', 1532),\n",
       " ('want_see', 1494),\n",
       " ('seem_like', 1473)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_counter.most_common(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZKzSOo6NA75k"
   },
   "source": [
    "The above bigrams might be frequent. However, you would not consider training individual embeddings for phrases such as *look_like* or *waste_time*. This shows how proper phrase detection in the scope of W2V is nontrivial and would require more work before we can hope to get good results.     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XILJmRqZtsFz"
   },
   "source": [
    "### Plotting word vectors\n",
    "It is fairly easy to create a visualization of the trained word vectors. You can find an example of how to do this in the [Kaggle kernel](https://www.kaggle.com/pierremegret/gensim-word2vec-tutorial) mentioned above. Needless to say, many alternative demos are available online; here is just [one example](https://www.kaggle.com/jeffd23/visualizing-word-vectors-with-t-sne). However, to get meaningful results we would need to prepare the data more carefully by, for example, removing too frequent words and too infrequent words. We would also finetune the training, and, overall, invest a lot more work to craft our word embeddings. In practice, we would typically not train our own embeddings from scratch. Instead, we would download pre-trained embeddings, which are available in many flavors (multiple languages, trained on different corpora with different jargon, etc.), and use these in our NLP application. We could also finetune the pre-trained embeddings using our own text data. We will showcase a corresponding approach in a later notebook on sentiment analysis. "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
